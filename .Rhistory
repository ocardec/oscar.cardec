HotelReviewsRaw[1]<-NULL
HotelReviewsRaw[11:12]<-NULL
HotelReviewsRaw[14:16]<-NULL
# Reviews rating as a factor
HotelReviewsRaw$reviews.rating <- factor(HotelReviewsRaw$reviews.rating)
# Concentrating into categories, reviews.text and reviews.title
textAndtitle<-filter(HotelReviewsRaw[c(1, 11:12)])
str(textAndtitle)
# get your sentiments in order, build your lexicon
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
# add index to your table
textAndtitle$index <- 1:nrow(textAndtitle)
head(textAndtitle)
# split sentences into words
wordfile<-strsplit(textAndtitle$reviews.text, " ")
head(wordfile)
# create corpus
textdoc <- Corpus(VectorSource(wordfile))
# modify content
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
textdoc <- tm_map(textdoc, toSpace, "/")
textdoc <- tm_map(textdoc, toSpace, "@")
textdoc <- tm_map(textdoc, toSpace, "\\|")
textdoc <- tm_map(textdoc, content_transformer(tolower))
# Remove punctuation
textdoc <- tm_map(textdoc, removePunctuation)
# Remove numbers
textdoc <- tm_map(textdoc, removeNumbers)
# Remove english common stopwords
textdoc <- tm_map(textdoc, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector
# textdoc <- tm_map(textdoc, removeWords, c("s", "company", "team"))
# Eliminate extra white spaces
textdoc <- tm_map(textdoc, stripWhitespace)
textdoc <- tm_map(textdoc, stemDocument)
# Display the top 5 most frequent words
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(textdoc)
TextDoc_dtm
dtm_m <- as.matrix(TextDoc_dtm)
dtm_m<-TextDoc_dtm
# Sort by descending value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
# Workspace Clean up
rm(list=ls())
#################################
# Data 630 – Section 9040 – Fall 2020 - Ami Gates
# Group Project - Module 2
#
# Vers Date       Author        Comments
# ---- ---------- ------------- ---------------------------------------------------
#  0.1 10/08/2020 K Eiholzer    Initial Version
#
#
# Prerequisites:
# 1. the files "Hotel_review.csv" has been placed in the input subdirectory directory from where this code is
#
#
#################################
# Source needed libraries and functions--------------------
# will un-comment if needed
# library(wordcloud)
# library(tm)
# library(maps)
# library(dplyr)
# library(usmap)
# library(ggplot2)
# library(quanteda)
# library(tokenizers)
#=====================================================================================================
# Organize Directories  and ingest data   ============================================================
#     set vairables for folder creation
source_file <- "Hotel_reviews_sentences_english.csv"
path_root <- getwd()
inputs_subdir <- "inputs"
outputs_subdir <- "outputs"
read_dir  <- file.path(path_root, inputs_subdir )
write_dir <- file.path(path_root, outputs_subdir)
#     create subdirectories to keep files tidy
try( dir.create(file.path(path_root, inputs_subdir)), silent = TRUE  )
try( dir.create(file.path(path_root, outputs_subdir)), silent = TRUE  )
#     move file to appropriate subdirectory
file.copy(source_file, read_dir)
ingest_file <- file.path(read_dir, source_file)
if (file.exists(ingest_file)) {
sentences_prepped_raw <- read.csv(ingest_file, head=TRUE, sep=",", as.is=FALSE)
} else {
print(paste0('ERROR File not found. Please ensure location of file here: ', ingest_file) )
}
getwd()
prereqs <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust",
"cluster", "igraph", "fpc")
install.packages(prereqs, dependencies = TRUE)
cname <- file.path("~", "Google Drive/R_Projects", "texts")
cname
dir(cname)
#
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
library(tm)
docs <- VCorpus(DirSource(cname))
summary(docs)
inspect(docs[1])
inspect(docs[2])
inspect(docs[3])
inspect(docs[4])
inspect(docs[1])
# Explore metadata of 1st document and total words
inspect(docs[1])
# Read all docs or look just one using index
writeLines(as.character(docs[1]))
len(docs[1])
# Explore metadata of 1st document and total characters
inspect(docs[1])
summary(docs)
# Read all docs or look just one using index
writeLines(as.character(docs[1]))
# Rm punctuation
docs <- tm_map(docs,removePunctuation)
# Rm numbers
docs <- tm_map(docs, removeNumbers)
# Rm special characters
for (j in seq(docs)) {
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])  # This is an ascii character that did not translate, so it had to be removed.
}
# to lowercase
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
DocsCopy <- docs
# For a list of the stopwords, see: length(stopwords("english"))
# stopwords("english")
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, PlainTextDocument)
# Rm particular words
docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))
# Combine words that most stay together
for (j in seq(docs)){
docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
# Stemming the docs (ie. removing "ing", "es", "s", etc.)
docs_st <- tm_map(docs, stemDocument)
docs_st <- tm_map(docs_st, PlainTextDocument)
# complete stemmed words
docs_stc <- tm_map(docs_st, stemCompletion, dictionary = DocsCopy, lazy=TRUE)
docs_stc <- tm_map(docs_stc, PlainTextDocument)
# complete stemmed words **** currently
docs_stc <- tm_map(docs_st, stemCompletion, dictionary = DocsCopy, type = c(prevalent))
# complete stemmed words **** currently
docs_stc <- tm_map(docs_st, stemCompletion, dictionary = DocsCopy, type = prevalent)
# complete stemmed words **** currently
docs_stc <- tm_map(docs_st, stemCompletion, dictionary = DocsCopy, type = "prevalent")
# complete stemmed words **** currently
docs_stc <- tm_map(docs_st, stemCompletion, dictionary = DocsCopy, type = "none")
# complete stemmed words **** currently
docs_stc <- tm_map(docs_st, stemCompletion, dictionary = DocsCopy, type = "longest")
# complete stemmed words **** currently
docs_stc <- tm_map(docs_st, stemCompletion)
# strip the whitespaces
docs <- tm_map(docs_st, stripWhitespace)
# Final step in cleaning
clean_docs <- tm_map(docs, PlainTextDocument)
# Create a Document Term Matrix
dtm <- DocumentTermMatrix(clean_docs)
dtm
dtm
# Transpose the matrix
tdm <- TermDocumentMatrix(docs)
tdm
#========================================================================#
# Organize terms by their frequency
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
# If you need to export as csv
m <- as.matrix(dtm)
dim(m)
str(m)
os.getwd()
os.getwd()
summary(m)
# Removing sparse terms
# This makes a matrix that is 20% empty space, maximum.
dtms <- removeSparseTerms(dtm, 0.2)
dtms
# Preview the most/least frequently occurred words; head and tail
freq <- colSums(as.matrix(dtm))
head(table(freq), 20)
tail(table(freq), 20)
# Macro preview
freq <- colSums(as.matrix(dtms))
freq
# Macro preview
freq <- order(colSums(as.matrix(dtms)))
freq
# Macro preview
freq <- colSums(as.matrix(dtms))
freq
ordered <- order(freq)
ordered
# Macro preview
freq <- colSums(order(as.matrix(dtms)))
# Macro preview
freq <- colSums(as.matrix(order(dtms)))
# Macro preview
freq <- colSums(as.matrix(dtms))
ordered <- order(freq)
ordered
freq
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
head(freq, 14)
findFreqTerms(dtm, lowfreq=50)
# terms frequeny 50 or more
findFreqTerms(dtm, lowfreq=50)
#========================================================================#
# Plot word frequencies
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
#========================================================================#
library(ggplot)
#========================================================================#
library(ggplot2)
# Plot word frequencies
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
# Plot word frequencies
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
p
# Plot word frequencies
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme_light(axis.text.x=element_text(angle=65, hjust=1))
# Plot word frequencies
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
# Plot word frequencies
b <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
b
# Plot word frequencies
b <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_histogram(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
b
# Plot word frequencies
b <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq), xlab="Ordered Words", ylab="Frequency") +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
# Plot word frequencies
b <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
# Plot word frequencies
b <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq), col=blues9) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
# Plot word frequencies
b <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity", col=blues9) +
theme(axis.text.x=element_text(angle=65, hjust=1))
# Plot word frequencies
b <- ggplot(subset(wf, freq>50), col=blues9, aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
# Plot word frequencies
b <- ggplot(subset(wf, freq>50),  aes(col=blues9, x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
# Plot word frequencies
b <- ggplot(subset(wf, freq>50),  aes(fill=blues9, x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
# Plot word frequencies
b <- ggplot(subset(wf, freq>50),aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
b
# Plot word frequencies
b <- ggplot(subset(wf, freq>50),aes(x = reorder(word, -freq), col=blues9, y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
b
# Plot word frequencies
b <- ggplot(subset(wf, freq>50),aes(x = reorder(word, -freq), fill=blues9, y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
b
# Plot word frequencies
b <- ggplot(subset(wf, freq>50),aes(x = reorder(word, -freq), fill=1:20, y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
b
# Plot word frequencies
b <- ggplot(subset(wf, freq>50),aes(x = reorder(word, -freq), y = freq), col="red") +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
b
# Plot word frequencies
b <- ggplot(subset(wf, freq>50),aes(x = reorder(word, -freq), y = freq), col="red") +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=.5))
b
# Plot word frequencies
b <- ggplot(subset(wf, freq>50),aes(x = reorder(word, -freq), y = freq), col="red") +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=.9))
b
# Plot word frequencies
b <- ggplot(subset(wf, freq>50),aes(x = reorder(word, -freq), y = freq), col="red") +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=65, hjust=1))
b
# find correlation betwen words
findAssocs(dtm, c("country" , "american"), corlimit=0.85)
findAssocs(dtms, "think", corlimit=0.70)
#========================================================================#
library(RColorBrewer)
set.seed(107)
wordcloud(names(freq), freq, min.freq=25)
dtm(names(freq), freq, min.freq=25)
dtms(names(freq), freq, min.freq=25)
#========================================================================#
library(RColorBrewer)
set.seed(107)
wordcloud::(names(freq), freq, min.freq=25)
wordcloud::names(freq), freq, min.freq=25)
wordcloud:names(freq), freq, min.freq=25)
wordcloud(names(freq), freq, min.freq=25)
wordcloud <- wf
wordcloud(names(freq), freq, min.freq=25)
set.seed(107)
wordcloud <- wf
wordcloud(names(freq), freq, min.freq=25)
wordcloud(wf(freq), freq, min.freq=25)
wordcloud <- dtm
wordcloud(wf(freq), freq, min.freq=25)
wordcloud(dtm(freq), freq, min.freq=25)
#========================================================================#
# install.packages("wordcloud")
# library(wordcloud)
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(RColorBrewer)
library(wordcloud2)
install.packages("wordcloud2")
set.seed(107)
wordcloud(names(freq), freq, min.freq=25)
wordcloud2(names(freq), freq, min.freq=25)
# Show the top 14 in descending order
head(freq, 14)
wordcloud2(words(freq), freq, min.freq=25)
# install.packages("wordcloud2")
library(wordcloud)
wordcloud(words(freq), freq, min.freq=25)
wordcloud(word(freq), freq, min.freq=25)
wordcloud(term(freq), freq, min.freq=25)
wordcloud(terms(freq), freq, min.freq=25)
wordcloud(names(freq), freq, min.freq=25)
wordcloud(names(freq), freq, min.freq=25, size = 1, shape = 'pentagon')
wordcloud(names(freq), freq, min.freq=25, size = 2, shape = 'pentagon')
wordcloud(names(freq), freq, min.freq=25, size = 3, shape = 'pentagon')
wordcloud(names(freq), freq, min.freq=25, size = .5, shape = 'pentagon')
wordcloud(names(freq), freq, min.freq=25, size = 1, shape = 'pentagon')
wordcloud(names(freq), freq, min.freq=15, size = 1, shape = 'pentagon')
wordcloud(names(freq), freq, min.freq=15, size = 2,
minRotation = -pi/2, maxRotation = -pi/2)
wordcloud(names(freq), freq, min.freq=15, size = 2, minRotation = -pi/6, maxRotation = -pi/6,
rotateRatio = 1)
wordcloud2(names(freq), freq, min.freq=15, size = 2, minRotation = -pi/6, maxRotation = -pi/6,
rotateRatio = 1)
wordcloud(names(freq), freq, min.freq=15, size = 2, minRotation = -pi/6, maxRotation = -pi/6,
rotateRatio = 1)
wordcloud2(data=names, size=1.6, color='random-dark')
wordcloud(data=names, size=1.6, color='random-dark')
wordcloud(data=wf, size=1.6, color='random-dark')
library(wordcloud2)
wordcloud(names(freq), freq, min.freq=15)
wordcloud(words = freq, freq = df$freq, min.freq = 1,
max.words=200, random.order=FALSE,
rot.per=0.35,colors=brewer.pal(8, "Dark2"))
wordcloud(words(freq), freq = df$freq, min.freq = 1,
max.words=200, random.order=FALSE,
rot.per=0.35,colors=brewer.pal(8, "Dark2"))
wordcloud(words(freq), freq = freq, min.freq = 1,
max.words=200, random.order=FALSE,
rot.per=0.35,colors=brewer.pal(8, "Dark2"))
summary(wf)
wordcloud2(data=wf, size=1.6, color='random-dark')
wordcloud2(data=wf, size=1, color='random-dark')
wordcloud2(data=wf, size = 0.7, color='random-dark', shape = 'pentagon')
wordcloud2(data=wf, size = 0.9, color='random-dark', shape = 'pentagon')
wordcloud2(data=wf, size = 0.9, color='random-dark', shape = 'pentagon')
set.seed(142)
wordcloud(names(freq), freq, min.freq=20, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
#========================================================================#
dtmss <- removeSparseTerms(dtm, 0.15)
dtmss
library(cluster)
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="complete")   # for a different look try substituting: method="ward.D"
fit
plot(fit, hang=-1)
fit <- hclust(d=d, method="ward.D")   # for a different look try substituting: method="ward.D"
fit
plot(fit, hang=-1)
fit <- hclust(d=d, method="complete") # or method="ward.D"
fit
plot(fit, hang=-1)
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=6)   # "k=" defines the number of clusters you are using
rect.hclust(fit, k=6, border="red") # draw dendogram with red borders around the 6 clusters
rect.hclust(fit, k=6, border="blue") # draw dendogram with red borders around the 6 clusters
library(fpc)
d <- dist(t(dtmss), method="euclidian")
kfit <- kmeans(d, 2)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
# Define prerequisites
prereqs <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust",
"cluster", "igraph", "fpc")
# Install the packages
install.packages(prereqs, dependencies = TRUE)
# Install special repo if needed
# install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
# map your path to the holding folder
cname <- file.path("~", "Google Drive/R_Projects", "texts")
dir(cname)
# Create volatile corpora
library(tm)
docs <- VCorpus(DirSource(cname))
summary(docs)
# Explore metadata of 1st document and total characters
inspect(docs[1])
# Read all docs or look just one using index
writeLines(as.character(docs[1]))
# Rm punctuation
docs <- tm_map(docs,removePunctuation)
# Rm numbers
docs <- tm_map(docs, removeNumbers)
# Rm special characters. Some ASCII characters do not necessarily translate, remove them here.
for (j in seq(docs)) {
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])
}
# to lowercase
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
DocsCopy <- docs
# For a list of the stopwords, see: length(stopwords("english"))
# stopwords("english")
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, PlainTextDocument)
# Rm particular words
docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))
# Combine words that most stay together
for (j in seq(docs)){
docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
# Stemming the docs (ie. removing "ing", "es", "s", etc.)
docs_st <- tm_map(docs, stemDocument)
docs_st <- tm_map(docs_st, PlainTextDocument)
# complete stemmed words **** currently not working ****
# docs_stc <- tm_map(docs_st, stemCompletion)
# docs_stc <- tm_map(docs_stc, PlainTextDocument)
# strip the whitespaces
docs <- tm_map(docs_st, stripWhitespace)
# Final step in cleaning
clean_docs <- tm_map(docs, PlainTextDocument)
#========================================================================#
# Create a Document Term Matrix
dtm <- DocumentTermMatrix(clean_docs)
dtm
# Transpose the matrix
tdm <- TermDocumentMatrix(docs)
tdm
tdm
# Removing sparse terms
# This makes a matrix that is 20% empty space, maximum.
dtms <- removeSparseTerms(dtm, 0.2)
dtms
colnames(dtms) <- make.names(colnames(dtms)) # Ensure the column names are appropriate for use in an R model
glimpse(dtms)
summary(dtms)
str(dtms)
install.packages(c("askpass", "backports", "BH", "bit", "bit64", "blob", "boot", "broom", "caret", "caTools", "class", "cli", "clipr", "codetools", "colorspace", "cpp11", "curl", "data.table", "DBI", "dbplyr", "digest", "dplyr", "evaluate", "fansi", "forcats", "foreach", "formatR", "generics", "gh", "glmnet", "glue", "gower", "haven", "hexbin", "hms", "htmltools", "htmlwidgets", "httpuv", "ipred", "IRkernel", "isoband", "ISOcodes", "iterators", "KernSmooth", "knitr", "labeling", "later", "lattice", "lava", "lubridate", "magrittr", "markdown", "MASS", "Matrix", "mclust", "mgcv", "mime", "ModelMetrics", "modelr", "mongolite", "multcomp", "nlme", "NLP", "nnet", "numDeriv", "odbc", "openssl", "pbdZMQ", "pillar", "pkgconfig", "PKI", "plyr", "prettyunits", "processx", "prodlim", "profvis", "progress", "promises", "ps", "quantmod", "R6", "RCurl", "readr", "recipes", "repr", "reprex", "reshape2", "rJava", "RJDBC", "RJSONIO", "rlang", "rmarkdown", "rprojroot", "rsconnect", "rvest", "scales", "selectr", "shiny", "slam", "SnowballC", "sparklyr", "spatial", "SQUAREM", "stopwords", "stringi", "sys", "testthat", "tibble", "tidyr", "tidyselect", "tidytext", "tidyverse", "tinytex", "tm", "TTR", "uuid", "whisker", "xfun", "xml2", "xts", "yaml", "zoo"))
install.packages(c("askpass", "backports", "BH", "bit", "bit64", "blob", "boot", "broom", "caret", "caTools", "class", "cli", "clipr", "codetools", "colorspace", "cpp11", "curl", "data.table", "DBI", "dbplyr", "digest", "dplyr", "evaluate", "fansi", "forcats", "foreach", "formatR", "generics", "gh", "glmnet", "glue", "gower", "haven", "hexbin", "hms", "htmltools", "htmlwidgets", "httpuv", "ipred", "IRkernel", "isoband", "ISOcodes", "iterators", "KernSmooth", "knitr", "labeling", "later", "lattice", "lava", "lubridate", "magrittr", "markdown", "MASS", "Matrix", "mclust", "mgcv", "mime", "ModelMetrics", "modelr", "mongolite", "multcomp", "nlme", "NLP", "nnet", "numDeriv", "odbc", "openssl", "pbdZMQ", "pillar", "pkgconfig", "PKI", "plyr", "prettyunits", "processx", "prodlim", "profvis", "progress", "promises", "ps", "quantmod", "R6", "RCurl", "readr", "recipes", "repr", "reprex", "reshape2", "rJava", "RJDBC", "RJSONIO", "rlang", "rmarkdown", "rprojroot", "rsconnect", "rvest", "scales", "selectr", "shiny", "slam", "SnowballC", "sparklyr", "spatial", "SQUAREM", "stopwords", "stringi", "sys", "testthat", "tibble", "tidyr", "tidyselect", "tidytext", "tidyverse", "tinytex", "tm", "TTR", "uuid", "whisker", "xfun", "xml2", "xts", "yaml", "zoo"))
install.packages(c("askpass", "backports", "BH", "bit", "bit64", "blob", "boot", "broom", "caret", "caTools", "class", "cli", "clipr", "codetools", "colorspace", "cpp11", "curl", "data.table", "DBI", "dbplyr", "digest", "dplyr", "evaluate", "fansi", "forcats", "foreach", "formatR", "generics", "gh", "glmnet", "glue", "gower", "haven", "hexbin", "hms", "htmltools", "htmlwidgets", "httpuv", "ipred", "IRkernel", "isoband", "ISOcodes", "iterators", "KernSmooth", "knitr", "labeling", "later", "lattice", "lava", "lubridate", "magrittr", "markdown", "MASS", "Matrix", "mclust", "mgcv", "mime", "ModelMetrics", "modelr", "mongolite", "multcomp", "nlme", "NLP", "nnet", "numDeriv", "odbc", "openssl", "pbdZMQ", "pillar", "pkgconfig", "PKI", "plyr", "prettyunits", "processx", "prodlim", "profvis", "progress", "promises", "ps", "quantmod", "R6", "RCurl", "readr", "recipes", "repr", "reprex", "reshape2", "rJava", "RJDBC", "RJSONIO", "rlang", "rmarkdown", "rprojroot", "rsconnect", "rvest", "scales", "selectr", "shiny", "slam", "SnowballC", "sparklyr", "spatial", "SQUAREM", "stopwords", "stringi", "sys", "testthat", "tibble", "tidyr", "tidyselect", "tidytext", "tidyverse", "tinytex", "tm", "TTR", "uuid", "whisker", "xfun", "xml2", "xts", "yaml", "zoo"))
remotes::install_github("seankross/postcards@main")
setwd("~/Documents/GitHub/ocardec.github.io")
setwd("~/Documents/GitHub/ocardec.github.io")
setwd("~/Documents/GitHub/ocardec.github.io/docs")
setwd("~/Documents/GitHub/ocardec.github.io")
